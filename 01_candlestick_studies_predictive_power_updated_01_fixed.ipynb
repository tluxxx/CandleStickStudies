{
  "nbformat": 4,
  "nbformat_minor": 5,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyN6clspHJhJ5zNRArVcLvaY",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/tluxxx/CandleStickStudies/blob/main/01_candlestick_studies_predictive_power_updated_01_fixed.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Candlestick Studies\n",
        "\n",
        "## Topics\n",
        "* Analyzing S&P 500 data from 2000 to 2025 for candlestick patterns using TA-Lib, generating bullish and bearish pattern signals\n",
        "* Introducing a simple EMA-based trend filter (and studying the impact)\n",
        "* Counting bullish/bearish signals and identifying the most active patterns\n",
        "* For each detected pattern (event/signal), compiling key information:\n",
        "    * date of occurrence,\n",
        "    * mode (bullish, bearish),\n",
        "    * market trend (no filter, up-trend, down-trend),\n",
        "    * and cumulative returns for the 20 days following the signal\n",
        "* Analyzing the distribution of cumulative returns\n",
        "* Rigorous statistical testing for:\n",
        "    * Mean cumulative returns after a pattern signal vs. mean cumulative returns after random entries\n",
        "    * Win rate after a pattern signal vs. win rate after random entries\n",
        "\n",
        "## Release Data\n",
        "* Rev: 1.0\n",
        "* Author: Tobi Lux\n",
        "* Updated: 2026-02-27\n",
        "\n"
      ],
      "metadata": {
        "id": "0Da_42e3sJWl"
      },
      "id": "0Da_42e3sJWl"
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 0. Preparation"
      ],
      "metadata": {
        "id": "LQ5YLffPJTNZ"
      },
      "id": "LQ5YLffPJTNZ"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5dZh2lx3sHwF"
      },
      "outputs": [],
      "source": [
        "print(\"â³ Downloading TA-Lib 0.6.4 from GitHub...\")\n",
        "!wget -q https://github.com/TA-Lib/ta-lib/releases/download/v0.6.4/ta-lib-0.6.4-src.tar.gz\n",
        "print(\"ðŸ“¦ Extracting...\")\n",
        "!tar -xzf ta-lib-0.6.4-src.tar.gz > /dev/null 2>&1\n",
        "print(\"ðŸ”§ Building TA-Lib C library (this takes 2-3 minutes)...\")\n",
        "!cd ta-lib-0.6.4 && ./configure --prefix=/usr > /dev/null 2>&1 && make > /dev/null 2>&1 && make install > /dev/null 2>&1\n",
        "print(\"ðŸ Installing Python wrapper...\")\n",
        "!pip install -q TA-Lib\n",
        "print(\"âœ… Done! TA-Lib 0.6.4 is ready to use!\")"
      ],
      "id": "5dZh2lx3sHwF"
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from typing import List, Any\n",
        "from tqdm.notebook import tqdm\n",
        "from numba import njit\n",
        "\n",
        "import yfinance as yf\n",
        "import talib\n",
        "from scipy import stats\n",
        "\n",
        "import plotly.graph_objects as go\n",
        "from plotly.subplots import make_subplots"
      ],
      "metadata": {
        "id": "Marb7S0Dshf6"
      },
      "execution_count": null,
      "outputs": [],
      "id": "Marb7S0Dshf6"
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1. Price Data Download and Trend Filter Calculation\n",
        "S&P 500 data are downloaded for the period from 2000 to 2025.\n",
        "\n",
        "As a trend filter we use a simple EMA(M). The filter is defined as:\n",
        "* +1 if Close > EMA(M)  â†’ uptrend\n",
        "* -1 if Close â‰¤ EMA(M)  â†’ downtrend\n",
        "\n",
        "Finally we calculate:\n",
        "* a series of trading days (all days in prices, no trend-filter applied)\n",
        "* a series of trading days in an up-trend (trend-filter == 1)\n",
        "* a series of trading days in a downtrend (trend-filter == -1)\n",
        "\n",
        "We will use these series of dates later for filtering."
      ],
      "metadata": {
        "id": "u_PKdkUQJahw"
      },
      "id": "u_PKdkUQJahw"
    },
    {
      "cell_type": "code",
      "source": [
        "# settings\n",
        "ticker = '^GSPC'\n",
        "start_date, end_date = '2000-01-01', '2025-12-31'\n",
        "N_FORWARD = 20\n",
        "\n",
        "# Download S&P 500 daily OHLC, caclulating daily returns\n",
        "prices = (yf.download(ticker, start=start_date, end=end_date, progress=False, auto_adjust=True)\n",
        "            [['Open','High','Low','Close']]\n",
        "            .droplevel(1, axis=1))\n",
        "rets = prices['Close'].pct_change().dropna()\n",
        "\n",
        "# EMA trend filter (+1 / -1)\n",
        "ema   = prices['Close'].ewm(span=50, adjust=False).mean()\n",
        "trend = (prices['Close'] > ema).map({True: 1, False: -1}).reindex(rets.index).rename('trend')\n",
        "\n",
        "# Date pools (exclude last N_FORWARD rows)\n",
        "valid = rets.index[:-N_FORWARD]\n",
        "dates_no_trend_filter   = valid                         # all datas\n",
        "dates_up_trend_filter   = valid[trend[valid] ==  1]     # dates in up-trend\n",
        "dates_down_trend_filter = valid[trend[valid] == -1]     # dates in down-trend"
      ],
      "metadata": {
        "id": "mpUUNfdcskk0"
      },
      "execution_count": null,
      "outputs": [],
      "id": "mpUUNfdcskk0"
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2. Detection of Candlestick Patterns\n",
        "* Identifying all candlestick patterns available in TA-Lib\n",
        "* Applying all available TA-Lib candlestick patterns to prices, generating a DataFrame `pattern_df` with all signals (signal = event = detection of a pattern)\n",
        "* Counting bearish and bullish signals (as per TA-Lib convention)\n",
        "* Counting all signals\n"
      ],
      "metadata": {
        "id": "lTqAM3JWJhZf"
      },
      "id": "lTqAM3JWJhZf"
    },
    {
      "cell_type": "code",
      "source": [
        "# extracting Candlestick pattern signals (CDL* in TA-Lib)\n",
        "cdl_patterns = talib.get_function_groups()['Pattern Recognition']\n",
        "\n",
        "pattern_df = pd.DataFrame(\n",
        "    {pat: getattr(talib, pat)(prices.Open, prices.High, prices.Low, prices.Close)\n",
        "     for pat in cdl_patterns},\n",
        "    index=prices.index\n",
        ")\n",
        "\n",
        "# Signal counts per pattern\n",
        "summary = pd.DataFrame({\n",
        "    'bull_sigs_talib': (pattern_df > 0).sum(),\n",
        "    'bear_sigs_talib': (pattern_df < 0).sum(),\n",
        "    'all_sigs_talib' : (pattern_df != 0).sum()\n",
        "}).sort_values('all_sigs_talib', ascending=False)"
      ],
      "metadata": {
        "id": "GTqDecZ0slpl"
      },
      "execution_count": null,
      "outputs": [],
      "id": "GTqDecZ0slpl"
    },
    {
      "cell_type": "markdown",
      "source": [
        "* Counting the number of detected signals per pattern\n",
        "* Plotting bar charts showing signal occurrences"
      ],
      "metadata": {
        "id": "znVHmZdYsgfi"
      },
      "id": "znVHmZdYsgfi"
    },
    {
      "cell_type": "code",
      "source": [
        "# Prepare data for plotting from summary\n",
        "plot_df = summary.reset_index()\n",
        "plot_df = plot_df.rename(columns={'index': 'Pattern'})\n",
        "\n",
        "# Create staggered bar chart\n",
        "fig = go.Figure(\n",
        "    data=[\n",
        "        go.Bar(x=plot_df['Pattern'], y=plot_df['bull_sigs_talib'],\n",
        "            name='Bullish Signals', marker_color='green'),\n",
        "        go.Bar(x=plot_df['Pattern'], y=plot_df['bear_sigs_talib'],\n",
        "            name='Bearish Signals', marker_color='red')\n",
        "        ]\n",
        ")\n",
        "# finetune layout\n",
        "fig.update_layout(\n",
        "    barmode='relative',\n",
        "    title_text= f'Nb of Bullish/Bearish Candlestick Signals (ticker {ticker}  from {start_date} to {end_date})',\n",
        "    xaxis_title='Candlestick Pattern', yaxis_title='Number of Signals',\n",
        "    title_font_size=16, # this line and below makes sur that all CSP are displayed\n",
        "    xaxis=dict(tickfont=dict(size=8),tickangle=-45, automargin=True),\n",
        "    yaxis=dict(tickfont=dict(size=10)),\n",
        "    legend=dict(font=dict(size=12)),\n",
        "    width=1000, height=400,\n",
        ")\n",
        "\n",
        "fig.show()"
      ],
      "metadata": {
        "id": "OfN-1TtuwoVj"
      },
      "execution_count": null,
      "outputs": [],
      "id": "OfN-1TtuwoVj"
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now we identify the most active patterns (measured by number of occurrences)."
      ],
      "metadata": {
        "id": "P0kqdsqZt4Cc"
      },
      "id": "P0kqdsqZt4Cc"
    },
    {
      "cell_type": "code",
      "source": [
        "# Filter patterns with >= N number of total signals\n",
        "N = 50\n",
        "\n",
        "top_patterns_talib = summary[summary['all_sigs_talib'] >= N].sort_values('all_sigs_talib', ascending=False)\n",
        "top_pattern_list   = top_patterns_talib.index.tolist()\n",
        "\n",
        "print(f'Frequent patterns (>= {N} signals): {len(top_patterns_talib)}')\n",
        "display(top_patterns_talib)"
      ],
      "metadata": {
        "id": "IlqFNLHQst2w"
      },
      "execution_count": null,
      "outputs": [],
      "id": "IlqFNLHQst2w"
    },
    {
      "cell_type": "code",
      "source": [
        "# list of top-active-patterns\n",
        "top_patterns_talib.index"
      ],
      "metadata": {
        "id": "H8GX3VdK0hlg"
      },
      "execution_count": null,
      "outputs": [],
      "id": "H8GX3VdK0hlg"
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3. Preparing Cumulative Return Data for Analysis and Random Entry Benchmark\n",
        "\n",
        "We compile now a dataframe `analytics` that stores for each detected pattern (=event or signal) the following information:\n",
        "* Date of occurrence\n",
        "* Pattern name\n",
        "* Mode (bullish, bearish)\n",
        "* Market trend (uptrend, downtrend), defined as Close > EMA(N) or Close â‰¤ EMA(N)\n",
        "* Cumulative returns for the 20 days following the signal\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "x-KIQ3BAKMu_"
      },
      "id": "x-KIQ3BAKMu_"
    },
    {
      "cell_type": "code",
      "source": [
        "# Event (= occurence of a pattern = signal) identification\n",
        "events = (\n",
        "    pattern_df[top_pattern_list]                        # top pattern only\n",
        "    .stack()                                            # rearranging to stack (row, column, value)\n",
        "    .reset_index()                                      # resetting index\n",
        "    .set_axis(['date', 'pattern', 'signal'], axis=1)    # renaming columns (unless df.columns=.. it is chainable)\n",
        ")\n",
        "\n",
        "# Keep signals only, convert to 1 (bullish) or -1 (bearish)\n",
        "events = events[events['signal'] != 0].copy()\n",
        "events['mode']   = np.sign(events['signal']).astype(int)\n",
        "\n",
        "# Add trend filter (bullish/bearish trend)\n",
        "events['trend'] = trend.loc[events['date']].to_numpy()\n",
        "\n",
        "# Map each date to its row index in rets, drop events too close to end of data\n",
        "row_pos           = pd.Series(np.arange(len(rets)), index=rets.index)\n",
        "events['row_pos'] = row_pos.loc[events['date']].to_numpy()\n",
        "events            = events[events['row_pos'] <= len(rets) - N_FORWARD - 1]\n",
        "\n",
        "# Collect forward returns for N_FORWARD days after each event\n",
        "offsets         = np.arange(1, N_FORWARD + 1)\n",
        "forward_returns = rets.values[events['row_pos'].to_numpy()[:, None] + offsets]\n",
        "\n",
        "# Build analytics dataframe combining event info and forward returns\n",
        "day_cols  = [f'Day_{i:02d}' for i in range(1, N_FORWARD + 1)]\n",
        "analytics = pd.concat([\n",
        "    events[['date', 'pattern', 'mode', 'trend']].reset_index(drop=True),\n",
        "    pd.DataFrame(forward_returns, columns=day_cols)\n",
        "], axis=1)\n",
        "\n",
        "# Prepend Day_00 = 0.0 as anchor (cumret starts at 1), then compute cumulative returns\n",
        "analytics.insert(analytics.columns.get_loc('trend') + 1, 'Day_00', 0.0)\n",
        "day_cols            = [f'Day_{i:02d}' for i in range(N_FORWARD + 1)]\n",
        "analytics[day_cols] = (1 + analytics[day_cols]).cumprod(axis=1)"
      ],
      "metadata": {
        "id": "nZo7k5IY-kUH"
      },
      "execution_count": null,
      "outputs": [],
      "id": "nZo7k5IY-kUH"
    },
    {
      "cell_type": "markdown",
      "source": [
        "Finally, we generate a sequence of 500 random entries and calculate the cumulative returns\n",
        "for the 20 days following each entry. Random entries are drawn under three conditions:\n",
        "* No trend filter â€” entries sampled freely across the full period\n",
        "* Uptrend filter â€” entries sampled only when Close > EMA(M)\n",
        "* Downtrend filter â€” entries sampled only when Close â‰¤ EMA(M)"
      ],
      "metadata": {
        "id": "y0cAlDYCJGLo"
      },
      "id": "y0cAlDYCJGLo"
    },
    {
      "cell_type": "code",
      "source": [
        "def compute_random_series(\n",
        "    valid_dates  : np.ndarray,\n",
        "    pattern_name : str,\n",
        "    rng          : np.random.Generator,\n",
        "    ) -> tuple[pd.DataFrame, pd.DataFrame]:\n",
        "\n",
        "    '''Randomly sample N entry dates and compute cumulative forward returns.\n",
        "    Args:\n",
        "        valid_dates:  Pool of eligible entry dates (pre-filtered for trend/lookahead).\n",
        "        pattern_name: Label assigned to this random benchmark series.\n",
        "        rng:          Random number generator (shared across calls to preserve state).\n",
        "    Returns:\n",
        "        analytics:   Per-event DataFrame with cumulative returns (Day_00 â€¦ Day_N).\n",
        "        summary_row: Single-row DataFrame with mean cumulative return per day.\n",
        "    '''\n",
        "    # Random selection of N entry dates from a pre-filtered set of dates\n",
        "    random_dates = rng.choice(valid_dates, size=min(N, len(valid_dates)), replace=False)\n",
        "\n",
        "    # Forward returns matrix (events x days)\n",
        "    rows = row_pos.loc[random_dates].to_numpy()\n",
        "    forward_returns = rets.values[rows[:, None] + offsets]\n",
        "\n",
        "    # Build per-event analytics with cumulative returns\n",
        "    cum_cols  = ['Day_00'] + day_cols\n",
        "    analytics = pd.concat([\n",
        "        pd.DataFrame({'date': random_dates, 'pattern': pattern_name}),\n",
        "        pd.DataFrame(forward_returns, columns=day_cols)\n",
        "    ], axis=1)\n",
        "    analytics.insert(analytics.columns.get_loc('pattern') + 1, 'Day_00', 0.0)\n",
        "    analytics[cum_cols] = (1 + analytics[cum_cols]).cumprod(axis=1)\n",
        "\n",
        "    # Summary: mean cumulative return per day\n",
        "    summary_row = pd.DataFrame({\n",
        "        'pattern'    : [pattern_name],\n",
        "        'occurrences': [len(analytics)],\n",
        "        'Day_00'     : [1.0],\n",
        "        **analytics[day_cols].mean().to_dict()\n",
        "    })\n",
        "\n",
        "    return analytics, summary_row\n"
      ],
      "metadata": {
        "id": "rvGqIaO-BDUJ"
      },
      "execution_count": null,
      "outputs": [],
      "id": "rvGqIaO-BDUJ"
    },
    {
      "cell_type": "code",
      "source": [
        "# Config\n",
        "SEED     = 42\n",
        "N        = 500\n",
        "rng      = np.random.default_rng(SEED)\n",
        "row_pos  = pd.Series(np.arange(len(rets)), index=rets.index)\n",
        "offsets  = np.arange(1, N_FORWARD + 1)\n",
        "day_cols = [f'Day_{i:02d}' for i in range(1, N_FORWARD + 1)]\n",
        "\n",
        "# Generate 3 random benchmarks (no trend filter / up-trend-filter / down trend-filter)\n",
        "analytics_rnd_no_trend, summary_no_trend   = compute_random_series(dates_no_trend_filter,   'RANDOM_no_trend_filter',   rng)\n",
        "analytics_rnd_up, summary_up   = compute_random_series(dates_up_trend_filter,   'RANDOM_up_trend_filter',   rng)\n",
        "analytics_rnd_down, summary_down = compute_random_series(dates_down_trend_filter, 'RANDOM_down_trend_filter', rng)\n",
        "\n",
        "# --- Combine details per-event in analytics df ---\n",
        "analytics_rnd_all = (\n",
        "    pd.concat([analytics_rnd_no_trend, analytics_rnd_up, analytics_rnd_down], ignore_index=True)\n",
        "    .sort_values(['pattern', 'date'])\n",
        "    .reset_index(drop=True)\n",
        ")\n",
        "\n",
        "# --- Combine summaries in summary df---\n",
        "random_summary = (\n",
        "    pd.concat([summary_no_trend, summary_up, summary_down], ignore_index=True)\n",
        "    [['pattern', 'occurrences', 'Day_00'] + day_cols]\n",
        ")"
      ],
      "metadata": {
        "id": "7nX6lcJK6mm0"
      },
      "execution_count": null,
      "outputs": [],
      "id": "7nX6lcJK6mm0"
    },
    {
      "cell_type": "code",
      "source": [
        "analytics_rnd_down.head(3)"
      ],
      "metadata": {
        "id": "rUhHmchvQTd9"
      },
      "execution_count": null,
      "outputs": [],
      "id": "rUhHmchvQTd9"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cb32d48c"
      },
      "source": [
        "random_summary"
      ],
      "execution_count": null,
      "outputs": [],
      "id": "cb32d48c"
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 4. cumulated returns over time, after detection of patterns"
      ],
      "metadata": {
        "id": "EBpOqGGzLEwP"
      },
      "id": "EBpOqGGzLEwP"
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 4.1 Helper functions"
      ],
      "metadata": {
        "id": "4nrg5AX__r6m"
      },
      "id": "4nrg5AX__r6m"
    },
    {
      "cell_type": "code",
      "source": [
        "def summary_cr(\n",
        "        analytics   : pd.DataFrame,\n",
        "        mode_value  : int,\n",
        "        trend_value : int,\n",
        "        day_cols    : List[str],\n",
        ") -> pd.DataFrame:\n",
        "\n",
        "    '''Compute mean cumulative returns and occurrence counts per pattern.\n",
        "    Args:\n",
        "        analytics:   Per-event DataFrame (output of main analytics cell).\n",
        "        mode_value:  Signal direction: 1 (bullish) or -1 (bearish).\n",
        "        trend_value: Trend filter: 1 (up-trend), -1 (down-trend), 0 (no filter).\n",
        "        day_cols:    Cumulative return columns to aggregate (Day_00 â€¦ Day_N).\n",
        "    Returns:\n",
        "        DataFrame with [pattern, mode, trend, occurrences] + day_cols,\n",
        "        sorted descending by the last day column.\n",
        "    '''\n",
        "    # Filter by mode, apply trend filter only if trend_value != 0\n",
        "    df = analytics[analytics['mode'] == mode_value]\n",
        "    if trend_value != 0:\n",
        "        df = df[df['trend'] == trend_value]\n",
        "\n",
        "    # Aggregate mean cumrets and occurrence counts per pattern\n",
        "    summary = (\n",
        "        df.groupby('pattern')[day_cols].mean()\n",
        "        .merge(df.groupby('pattern').size().rename('occurrences'), left_index=True, right_index=True)\n",
        "        .reset_index()\n",
        "    )\n",
        "    summary['mode']  = mode_value\n",
        "    summary['trend'] = trend_value\n",
        "\n",
        "    return (summary[['pattern', 'mode', 'trend', 'occurrences'] + day_cols]\n",
        "            .sort_values(day_cols[-1], ascending=False)\n",
        "            .reset_index(drop=True))\n"
      ],
      "metadata": {
        "id": "-QmY3zCcZRfT"
      },
      "execution_count": null,
      "outputs": [],
      "id": "-QmY3zCcZRfT"
    },
    {
      "cell_type": "code",
      "source": [
        "def plot_trajectories(\n",
        "        traject          : pd.DataFrame,\n",
        "        random_benchmark : pd.DataFrame,\n",
        "        cum_cols         : List[str],\n",
        "        title_suffix     : str  = \"\",\n",
        "        style_map        : dict = None,\n",
        ") -> go.Figure:\n",
        "\n",
        "    ''' Plot averaged cumulative return trajectories with random benchmarks.\n",
        "    Args:\n",
        "        traject:          Output of summary_cr(), mean cumrets per pattern/day.\n",
        "        random_benchmark: random_summary DataFrame (3 benchmark rows).\n",
        "        cum_cols:         Day columns to plot (Day_00 â€¦ Day_20).\n",
        "        title_suffix:     Appended to the plot title.\n",
        "        style_map:        Maps raw benchmark pattern names to display label and line color.\n",
        "                          Format: {\"RANDOM_key\": {\"label\": \"...\", \"color\": \"...\"}}.\n",
        "    Returns:\n",
        "        plotly go.Figure\n",
        "    '''\n",
        "    fig = go.Figure()\n",
        "\n",
        "    # Signal trajectories\n",
        "    for _, row in traject.iterrows():\n",
        "        fig.add_trace(go.Scatter(\n",
        "            x=cum_cols, y=row[cum_cols],\n",
        "            mode=\"lines\", name=row[\"pattern\"], line=dict(width=1)\n",
        "        ))\n",
        "\n",
        "    # Random benchmark trajectories\n",
        "    for _, row in random_benchmark.iterrows():\n",
        "        raw_name = row[\"pattern\"]\n",
        "        style    = style_map.get(raw_name, {}) if style_map else {}\n",
        "        fig.add_trace(go.Scatter(\n",
        "            x=cum_cols, y=row[cum_cols],\n",
        "            mode=\"lines\",\n",
        "            name=style.get(\"label\", raw_name),\n",
        "            line=dict(width=3, color=style.get(\"color\", \"black\"), dash=\"dashdot\")\n",
        "        ))\n",
        "\n",
        "    fig.update_layout(\n",
        "        title=f\"Cumulative Return Trajectories {title_suffix}\",\n",
        "        template=PLOT_TEMPLATE, height=PLOT_HEIGHT, width=PLOT_WIDTH,\n",
        "    )\n",
        "    fig.update_xaxes(title=\"Holding Period\", tickangle=-90, showgrid=True)\n",
        "    fig.update_yaxes(title=\"Cumulative Return\", showgrid=True)\n",
        "\n",
        "    return fig\n",
        "\n"
      ],
      "metadata": {
        "id": "oVd3qkogLcHY"
      },
      "execution_count": null,
      "outputs": [],
      "id": "oVd3qkogLcHY"
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 4.2. Trajectories of Cumulative Returns\n",
        "We plot the evolution of the average cumulative return over the 20 days following a pattern\n",
        "signal, broken down by pattern, mode (bullish/bearish), and trend filter\n",
        "(no filter, uptrend, downtrend)."
      ],
      "metadata": {
        "id": "Z4MhiLM56YKZ"
      },
      "id": "Z4MhiLM56YKZ"
    },
    {
      "cell_type": "code",
      "source": [
        "# principal settings\n",
        "\n",
        "RANDOM_BENCHMARK_STYLE = {\n",
        "    \"RANDOM_no_trend_filter\"  : {\"regime\":  0, \"label\": \"Random entries (no trend filter)\", \"color\": \"yellow\"},\n",
        "    \"RANDOM_up_trend_filter\"  : {\"regime\":  1, \"label\": \"Random entries (in up-trends)\",    \"color\": \"red\"   },\n",
        "    \"RANDOM_down_trend_filter\": {\"regime\": -1, \"label\": \"Random entries (in down-trends)\",  \"color\": \"green\" },\n",
        "}\n",
        "\n",
        "# Derived lookup: regime integer -> pattern key\n",
        "REGIME_TO_RANDOM = {v[\"regime\"]: k for k, v in RANDOM_BENCHMARK_STYLE.items()}\n",
        "\n",
        "combinations = [\n",
        "    {'mode': 1, 'trend': 0, 'description': 'pure bullish signals, no filter'},\n",
        "    {'mode':-1, 'trend': 0, 'description': 'pure bearish signals, no filter'},\n",
        "    {'mode': 1, 'trend': 1, 'description': 'bullish signals in up-trends'},\n",
        "    {'mode': 1, 'trend':-1, 'description': 'bullish signals in down-trends'},\n",
        "    {'mode':-1, 'trend':-1, 'description': 'bearish signals in down-trends'},\n",
        "    {'mode':-1, 'trend': 1, 'description': 'bearish signals in up-trends'},\n",
        "]\n",
        "\n",
        "PLOT_TEMPLATE = \"plotly_dark\"\n",
        "PLOT_HEIGHT = 450\n",
        "PLOT_WIDTH = 800\n",
        "\n",
        "TOP_N    = 10\n",
        "all_days = ['Day_00'] + [f'Day_{i:02d}' for i in range(1, N_FORWARD + 1)]\n"
      ],
      "metadata": {
        "id": "r5AxPjAfMq-z"
      },
      "execution_count": null,
      "outputs": [],
      "id": "r5AxPjAfMq-z"
    },
    {
      "cell_type": "code",
      "source": [
        "# plotting\n",
        "\n",
        "for comb in combinations:\n",
        "    mode, trend, desc = comb[\"mode\"], comb[\"trend\"], comb[\"description\"]\n",
        "\n",
        "    traject = summary_cr(analytics, mode_value=mode, trend_value=trend, day_cols=all_days)\n",
        "    traject = traject.tail(TOP_N) if \"bearish\" in desc else traject.head(TOP_N)\n",
        "\n",
        "    plot_trajectories(\n",
        "        traject          = traject,\n",
        "        random_benchmark = random_summary,\n",
        "        cum_cols         = all_days,\n",
        "        title_suffix     = f\"â€” {desc}\",\n",
        "        style_map        = RANDOM_BENCHMARK_STYLE,\n",
        "    ).show()\n"
      ],
      "metadata": {
        "id": "B4avLqibLpKx"
      },
      "execution_count": null,
      "outputs": [],
      "id": "B4avLqibLpKx"
    },
    {
      "cell_type": "code",
      "source": [
        "# selection of days for further analytics\n",
        "selected_days = [1, 5, 10, 15, 20]\n",
        "selected_day_cols = [f\"Day_{i:02d}\" for i in selected_days]"
      ],
      "metadata": {
        "id": "xp9WyQCr8WyY"
      },
      "execution_count": null,
      "outputs": [],
      "id": "xp9WyQCr8WyY"
    },
    {
      "cell_type": "code",
      "source": [
        "# just for demonstration: option to print results for selected days\n",
        "N = 10          # Top N\n",
        "mode = 1        # bullish\n",
        "trend = 1       # up-trend filter\n",
        "summary = summary_cr(analytics, mode_value=1, trend_value=0, day_cols=selected_day_cols).head(N) # use tail for bearish\n",
        "summary"
      ],
      "metadata": {
        "id": "3-uJ6T52YPTh"
      },
      "execution_count": null,
      "outputs": [],
      "id": "3-uJ6T52YPTh"
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 5. Distribution of cumulated returns after detection of a pattern"
      ],
      "metadata": {
        "id": "zP2DMsa-_KR2"
      },
      "id": "zP2DMsa-_KR2"
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 5.1. Helper functions"
      ],
      "metadata": {
        "id": "WY8wrVxjCXrn"
      },
      "id": "WY8wrVxjCXrn"
    },
    {
      "cell_type": "code",
      "source": [
        "def get_cumrets_for_pattern(\n",
        "        analytics    : pd.DataFrame,\n",
        "        pattern_name : str,\n",
        "        mode_value   : int,\n",
        "        trend_value  : int,\n",
        "        day_col      : str,\n",
        ") -> pd.Series:\n",
        "\n",
        "    '''Extract cumulative returns for a given pattern, mode and trend filter.\n",
        "    Args:\n",
        "        analytics:    Per-event DataFrame with pattern, mode, trend and day columns.\n",
        "        pattern_name: Candlestick pattern name (e.g. 'CDLSPINNINGTOP').\n",
        "        mode_value:   Signal direction: 1 (bullish) or -1 (bearish).\n",
        "        trend_value:  Trend filter: 1 (up-trend), -1 (down-trend), 0 (no filter).\n",
        "        day_col:      Cumulative return column to extract (e.g. 'Day_20').\n",
        "    Returns:\n",
        "        pd.Series of cumulative returns for the selected pattern/mode/trend.\n",
        "    '''\n",
        "    mask = (analytics['pattern'] == pattern_name) & (analytics['mode'] == mode_value)\n",
        "    if trend_value != 0:\n",
        "        mask &= (analytics['trend'] == trend_value)\n",
        "\n",
        "    return analytics.loc[mask, day_col].dropna()\n",
        "\n",
        "\n",
        "def get_random_cumrets(\n",
        "        analytics_rnd_all : pd.DataFrame,\n",
        "        trend_value       : int,\n",
        "        day_col           : str,\n",
        ") -> pd.Series:\n",
        "\n",
        "    '''Extract cumulative returns from the random benchmark matching the trend regime.\n",
        "    Args:\n",
        "        analytics_rnd_all: Concatenated random analytics DataFrame.\n",
        "        trend_value:       Regime: 0 (no filter), 1 (up-trend), -1 (down-trend).\n",
        "        day_col:           Cumulative return column to extract (e.g. 'Day_20').\n",
        "    Returns:\n",
        "        pd.Series of cumulative returns for the matching random benchmark.\n",
        "    '''\n",
        "    if trend_value not in REGIME_TO_RANDOM:\n",
        "        raise ValueError(f\"trend_value must be 0, 1, or -1, got {trend_value}\")\n",
        "\n",
        "    return analytics_rnd_all.loc[\n",
        "        analytics_rnd_all[\"pattern\"] == REGIME_TO_RANDOM[trend_value], day_col\n",
        "    ].dropna()"
      ],
      "metadata": {
        "id": "i-cORsTn_3t1"
      },
      "execution_count": null,
      "outputs": [],
      "id": "i-cORsTn_3t1"
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 5.2. visualisation of distribution"
      ],
      "metadata": {
        "id": "ixGI-ETICevm"
      },
      "id": "ixGI-ETICevm"
    },
    {
      "cell_type": "code",
      "source": [
        "# --- Parameters ---\n",
        "pattern_name  = 'CDLSPINNINGTOP'\n",
        "mode_value    = 1\n",
        "trend_value   = 1      # 1=up-trend, -1=down-trend, 0=no filter\n",
        "selected_days = [5, 10, 15, 20]\n",
        "day_cols_sel  = [f'Day_{d:02d}' for d in selected_days]\n",
        "\n",
        "# --- Benchmark style ---\n",
        "rnd_key   = REGIME_TO_RANDOM[trend_value]\n",
        "rnd_label = RANDOM_BENCHMARK_STYLE[rnd_key][\"label\"]\n",
        "rnd_color = RANDOM_BENCHMARK_STYLE[rnd_key][\"color\"]\n",
        "\n",
        "# --- Build 2x2 subplots ---\n",
        "fig = make_subplots(\n",
        "    rows=2, cols=2,\n",
        "    subplot_titles=[f\"Day {d}\" for d in selected_days],\n",
        "    shared_xaxes=False, shared_yaxes=False,\n",
        "    vertical_spacing=0.12, horizontal_spacing=0.08\n",
        ")\n",
        "\n",
        "positions = [(1,1), (1,2), (2,1), (2,2)]\n",
        "\n",
        "for (row, col), (d, day_col) in zip(positions, zip(selected_days, day_cols_sel)):\n",
        "    cumrets        = get_cumrets_for_pattern(analytics, pattern_name, mode_value, trend_value, day_col)\n",
        "    cumrets_random = get_random_cumrets(analytics_rnd_all, trend_value, day_col)\n",
        "\n",
        "    show_rnd_legend = (row == 1 and col == 1)   # random label only once\n",
        "\n",
        "    fig.add_trace(go.Histogram(\n",
        "        x=cumrets,\n",
        "        name=f\"{pattern_name} | Day {d}\",          # unique label per subplot\n",
        "        histnorm='probability density', opacity=0.8,\n",
        "        showlegend=True                             # always show pattern label\n",
        "    ), row=row, col=col)\n",
        "\n",
        "    fig.add_trace(go.Histogram(\n",
        "        x=cumrets_random,\n",
        "        name=rnd_label,\n",
        "        histnorm='probability density', opacity=0.5,\n",
        "        marker_color=rnd_color,\n",
        "        showlegend=show_rnd_legend                  # random label only once\n",
        "    ), row=row, col=col)\n",
        "\n",
        "fig.update_layout(\n",
        "    barmode='overlay',\n",
        "    title=f\"Cumulative Return Distribution â€” {pattern_name} | Trend={trend_value}\",\n",
        "    template=PLOT_TEMPLATE,\n",
        "    height=PLOT_HEIGHT * 1.5,\n",
        "    width=PLOT_WIDTH  * 1.5,\n",
        ")\n",
        "\n",
        "# x-axis label only on bottom row (row 2)\n",
        "for i in range(1, 5):\n",
        "    r = 1 if i <= 2 else 2\n",
        "    c = (i - 1) % 2 + 1\n",
        "    if r == 2:\n",
        "        fig.update_xaxes(title_text=\"Cumulative Return\", row=r, col=c)\n",
        "    fig.update_yaxes(title_text=\"Density\", row=r, col=c)\n",
        "\n",
        "fig.show()"
      ],
      "metadata": {
        "id": "sh4buFl7BinK"
      },
      "execution_count": null,
      "outputs": [],
      "id": "sh4buFl7BinK"
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 6. Statistical tests"
      ],
      "metadata": {
        "id": "Cubu0oMsGTWv"
      },
      "id": "Cubu0oMsGTWv"
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 6.1 Test for Normality of Cumulative Returns\n",
        "\n",
        "We test whether the distribution of cumulative returns at selected holding periods follows\n",
        "a normal distribution. This confirms our choice of non-parametric tests for the subsequent\n",
        "significance testing.\n",
        "\n",
        "**Result:** The distribution of cumulative returns is not normal."
      ],
      "metadata": {
        "id": "XnFW_2lETSkv"
      },
      "id": "XnFW_2lETSkv"
    },
    {
      "cell_type": "code",
      "source": [
        "# hypothesis\n",
        "H0 = \"The cumulative returns at the selected day follow a normal distribution.\"\n",
        "print(f\"H0: {H0}\\n\")\n",
        "\n",
        "# --- Parameters ---\n",
        "selected_days = [5, 10, 15, 20]\n",
        "day_cols_sel  = [f'Day_{d:02d}' for d in selected_days]\n",
        "ALPHA         = 0.05\n",
        "N_min         = 30 # minum number of patterns for testing\n",
        "\n",
        "# --- Full universe screen across selected days ---\n",
        "results = []\n",
        "\n",
        "for day_col in day_cols_sel:\n",
        "    for comb in combinations:\n",
        "        mode_value  = comb[\"mode\"]\n",
        "        trend_value = comb[\"trend\"]\n",
        "\n",
        "        sig_col  = 'bull_sigs_talib' if mode_value == 1 else 'bear_sigs_talib'\n",
        "        patterns = top_patterns_talib[top_patterns_talib[sig_col] > N_min].index\n",
        "\n",
        "        for pattern_name in patterns:\n",
        "            cumrets = get_cumrets_for_pattern(analytics, pattern_name, mode_value, trend_value, day_col)\n",
        "            if len(cumrets) < 3:\n",
        "                continue\n",
        "            _, p_value = stats.shapiro(cumrets)\n",
        "            results.append({\n",
        "                'day'    : day_col,\n",
        "                'pattern': pattern_name,\n",
        "                'mode'   : mode_value,\n",
        "                'trend'  : trend_value,\n",
        "                'n'      : len(cumrets),\n",
        "                'p_value': round(p_value, 4),\n",
        "                'verdict': 'Fail to reject H0' if p_value > ALPHA else 'Reject H0',\n",
        "            })\n",
        "\n",
        "normality_results = (\n",
        "    pd.DataFrame(results)\n",
        "    .sort_values(['day', 'mode', 'trend', 'p_value'], ascending=[True, True, True, False])\n",
        "    .reset_index(drop=True)\n",
        ")\n",
        "\n",
        "# --- Summary: rejection counts per day and combination ---\n",
        "summary_normality = (\n",
        "    normality_results\n",
        "    .groupby(['day', 'mode', 'trend', 'verdict'])\n",
        "    .size()\n",
        "    .unstack(fill_value=0)\n",
        "    .rename_axis(None, axis=1)\n",
        "    .reset_index()\n",
        ")\n",
        "for col in ['Reject H0', 'Fail to reject H0']:\n",
        "    if col not in summary_normality.columns:\n",
        "        summary_normality[col] = 0\n",
        "\n",
        "summary_normality['total']      = summary_normality['Reject H0'] + summary_normality['Fail to reject H0']\n",
        "summary_normality['pct_reject'] = (summary_normality['Reject H0'] / summary_normality['total'] * 100).round(1)\n",
        "\n",
        "display(summary_normality)"
      ],
      "metadata": {
        "id": "Ldm_1TpNTdtr"
      },
      "execution_count": null,
      "outputs": [],
      "id": "Ldm_1TpNTdtr"
    },
    {
      "cell_type": "code",
      "source": [
        "# --- Patterns failing to reject H0 on ALL selected days ---\n",
        "fail_to_reject = normality_results[normality_results['verdict'] == 'Fail to reject H0']\n",
        "\n",
        "# Keep only patterns present in all selected days\n",
        "pattern_day_counts   = fail_to_reject.groupby('pattern')['day'].nunique()\n",
        "patterns_all_days    = pattern_day_counts[pattern_day_counts == len(selected_days)].index\n",
        "\n",
        "print(f\"H0: {H0}\\n\")\n",
        "print(f\"Patterns failing to reject H0 on all {len(selected_days)} selected days: {len(patterns_all_days)}\\n\")\n",
        "\n",
        "for pattern in patterns_all_days:\n",
        "    subset = (\n",
        "        fail_to_reject[fail_to_reject['pattern'] == pattern]\n",
        "        [['day', 'mode', 'trend', 'n', 'p_value', 'verdict']]\n",
        "        .sort_values(['day', 'mode', 'trend'])\n",
        "        .reset_index(drop=True)\n",
        "    )\n",
        "    print(f\"Pattern: {pattern}\")\n",
        "    # display(subset)\n",
        "    #print()"
      ],
      "metadata": {
        "id": "n3_sqtELZm4E"
      },
      "execution_count": null,
      "outputs": [],
      "id": "n3_sqtELZm4E"
    },
    {
      "cell_type": "code",
      "source": [
        "# --- Normality test (Jarque-Bera) for random benchmark cumrets across selected days ---\n",
        "results_random = []\n",
        "\n",
        "for day_col in day_cols_sel:\n",
        "    for trend_value, rnd_key in REGIME_TO_RANDOM.items():\n",
        "        cumrets = get_random_cumrets(analytics_rnd_all, trend_value, day_col)\n",
        "        if len(cumrets) < 3:\n",
        "            continue\n",
        "        stat, p_value = stats.jarque_bera(cumrets)\n",
        "        results_random.append({\n",
        "            'day'    : day_col,\n",
        "            'pattern': rnd_key,\n",
        "            'trend'  : trend_value,\n",
        "            'n'      : len(cumrets),\n",
        "            'stat'   : round(stat, 4),\n",
        "            'p_value': p_value,\n",
        "            'verdict': 'Fail to reject H0' if p_value > ALPHA else 'Reject H0',\n",
        "        })\n",
        "\n",
        "normality_results_random = (\n",
        "    pd.DataFrame(results_random)\n",
        "    .sort_values(['day', 'trend'])\n",
        "    .reset_index(drop=True)\n",
        ")\n",
        "\n",
        "print(f\"H0: {H0}\\n\")\n",
        "display(normality_results_random.style.format({'p_value': '{:.2e}', 'stat': '{:.4f}'}))"
      ],
      "metadata": {
        "id": "S5NXYR-VbZZL"
      },
      "execution_count": null,
      "outputs": [],
      "id": "S5NXYR-VbZZL"
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 6.2. Test for equal means and equal win rates"
      ],
      "metadata": {
        "id": "fvfppdGMNJZh"
      },
      "id": "fvfppdGMNJZh"
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 6.2.1. Helper Functions\n",
        "\n",
        "The helper functions support the following tasks:\n",
        "* Checking whether a sufficient number of occurrences exists for meaningful statistical testing\n",
        "* Calculating the win rate of a pattern depending on signal mode (bullish signals require cumrets > 1, bearish signals require cumrets < 1)"
      ],
      "metadata": {
        "id": "0NPFacZ-BDp-"
      },
      "id": "0NPFacZ-BDp-"
    },
    {
      "cell_type": "code",
      "source": [
        "def data_quality(n: int) -> str:\n",
        "    if n > 100:   return 'good'\n",
        "    elif n >= 30: return 'acceptable'\n",
        "    else:         return 'indication only'\n",
        "\n",
        "def win_condition(cumrets: np.ndarray, mode_value: int) -> np.ndarray:\n",
        "    ''' defines what is a winner: for a bullish signal cumrets > 1 wins, for a bearish signal cumrets < 1 win\n",
        "    Args:\n",
        "        cumrets: cumulative returns\n",
        "        mode_value: 1 (bullish) or -1 (bearish)\n",
        "    Returns:\n",
        "        np.ndarray of booleans\n",
        "    '''\n",
        "    return cumrets > 1 if (mode_value == 1 ) else cumrets < 1\n",
        "\n",
        "def wr_direction(wr_diff: float) -> str:\n",
        "    ''' defines the direction of the win rate '''\n",
        "    return 'over' if wr_diff > 0 else 'under'"
      ],
      "metadata": {
        "id": "kI14mogV3T2a"
      },
      "execution_count": null,
      "outputs": [],
      "id": "kI14mogV3T2a"
    },
    {
      "cell_type": "markdown",
      "source": [
        "We test the following hypotheses:\n",
        "* **H0_Âµ**: The mean cumulative return after a pattern signal equals the mean cumulative return after a random entry (Âµ_pattern = Âµ_random)\n",
        "* **H0_wr**: The win rate after a pattern signal equals the win rate after a random entry (wr_pattern = wr_random)\n",
        "\n",
        "For each pattern we compare its cumulative returns â€” filtered by mode (bullish/bearish) and\n",
        "trend (no filter, uptrend, downtrend) â€” against the cumulative returns of a random entry\n",
        "series filtered by the same trend condition.\n",
        "\n",
        "The tested series have the following properties:\n",
        "* Non-normally distributed\n",
        "* Independent\n",
        "* Unequal sample sizes\n",
        "\n",
        "The most robust and suitable test under these conditions is the permutation test.\n",
        "\n",
        "**H0_Âµ** is tested using a **mean-centered permutation test**. Mean-centering imposes the\n",
        "null hypothesis (Âµ_pattern = Âµ_random) before pooling, ensuring the permuted null distribution\n",
        "correctly reflects \"no difference in means\".\n",
        "\n",
        "**H0_wr** is tested using a **standard (non-centered) permutation test**. Win rates are binary\n",
        "series (0/1), and centering would destroy the binary structure and invalidate the null\n",
        "distribution. Pooling the raw 0/1 values and reshuffling correctly reflects \"no difference\n",
        "in win rates\".\n"
      ],
      "metadata": {
        "id": "RyqpWXvG-JkP"
      },
      "id": "RyqpWXvG-JkP"
    },
    {
      "cell_type": "code",
      "source": [
        "def permutation_test_centered(\n",
        "    sample: np.ndarray,\n",
        "    reference: np.ndarray,\n",
        "    n_perm: int = 10_000,\n",
        "    rng: np.random.Generator=None,\n",
        "    ) -> tuple[float, float]:\n",
        "\n",
        "    ''' Permutation test: Version mean-centric-permutation (test for Âµ1=Âµ2) and continuous values\n",
        "    Args:\n",
        "        sample: sample 1\n",
        "        reference: sample 2 (reference)\n",
        "        n_perm:number of permutations\n",
        "        rng: random number generator\n",
        "    Returns as tuple (,):\n",
        "        T_obs: observed difference\n",
        "        p_value: p-value\n",
        "    '''\n",
        "    n = len(sample)\n",
        "    obs = sample.mean() - reference.mean()   # observation (T_obs)\n",
        "\n",
        "    # make tw samples centric and form the urn (=pool)\n",
        "    pool = np.concatenate([sample - sample.mean(), reference - reference.mean()])\n",
        "\n",
        "    # Monte Carlo-Simulation (n_perm-permutations, slicing at n and calculating the difference of the means)\n",
        "    perms  = np.array([\n",
        "        (perm := rng.permutation(pool))[:n].mean() - perm[n:].mean()\n",
        "        for _ in range(n_perm)\n",
        "    ])\n",
        "\n",
        "    return obs, np.mean(np.abs(perms) >= np.abs(obs)) # boolean array for the difference, then averaging (True =1, False=0)\n",
        "\n",
        "def permutation_test_standard(\n",
        "    sample: np.ndarray,\n",
        "    reference: np.ndarray,\n",
        "    n_perm: int = 10_000,\n",
        "    rng: np.random.Generator = None,\n",
        "    ) -> tuple[float, float]:\n",
        "\n",
        "    ''' Standard Permutation test: pool-and-reshuffle (no mean-centering) (tests p1=p2).\n",
        "        suitable also for binary series (as win-rates), preserves binary 0/1 structure.\n",
        "    Args:\n",
        "        sample: sample 1\n",
        "        reference: sample 2 (reference)\n",
        "        n_perm: number of permutations\n",
        "        rng: random number generator\n",
        "    Returns as tuple (,):\n",
        "        T_obs: observed win rate difference (sample - reference)\n",
        "        p_value: p-value\n",
        "    '''\n",
        "    n   = len(sample)\n",
        "    obs = sample.mean() - reference.mean()\n",
        "\n",
        "    # pool raw 0/1 values â€” no centering\n",
        "    pool = np.concatenate([sample, reference])\n",
        "\n",
        "    # Monte Carlo simulation\n",
        "    perms = np.array([\n",
        "        (perm := rng.permutation(pool))[:n].mean() - perm[n:].mean()\n",
        "        for _ in range(n_perm)\n",
        "    ])\n",
        "\n",
        "    return obs, np.mean(np.abs(perms) >= np.abs(obs))\n"
      ],
      "metadata": {
        "id": "c8ccN80E3aor"
      },
      "execution_count": null,
      "outputs": [],
      "id": "c8ccN80E3aor"
    },
    {
      "cell_type": "markdown",
      "source": [
        "To speed up execution, both tests are implemented using explicit loops with optimized\n",
        "random number generation, accelerated by Numba JIT compilation."
      ],
      "metadata": {
        "id": "Pc6hLGJlA6zX"
      },
      "id": "Pc6hLGJlA6zX"
    },
    {
      "cell_type": "code",
      "source": [
        "# permutations using numba functionality for acceleration\n",
        "@njit\n",
        "def _permutation_test_centered_numba(a, b, n_perm, seed):\n",
        "    \"\"\"Mean-centered permutation test for continuous data (tests Âµ1 = Âµ2).\"\"\"\n",
        "    na = len(a)\n",
        "    nb = len(b)\n",
        "    n  = na + nb\n",
        "\n",
        "    # center both samples (impose H0: Âµ1 = Âµ2)\n",
        "    mean_a = a.mean()\n",
        "    mean_b = b.mean()\n",
        "\n",
        "    combined = np.empty(n)\n",
        "    for i in range(na):\n",
        "        combined[i] = a[i] - mean_a\n",
        "    for i in range(nb):\n",
        "        combined[na + i] = b[i] - mean_b\n",
        "\n",
        "    T_obs     = mean_a - mean_b\n",
        "    abs_T_obs = T_obs if T_obs >= 0.0 else -T_obs  # abs(T_obs)\n",
        "\n",
        "    rng_state = seed\n",
        "    count = 0\n",
        "\n",
        "    for _ in range(n_perm):\n",
        "        for i in range(n - 1, 0, -1):\n",
        "            rng_state = (rng_state * 6364136223846793005 + 1442695040888963407) & 0xFFFFFFFFFFFFFFFF\n",
        "            j = rng_state % (i + 1)\n",
        "            combined[i], combined[j] = combined[j], combined[i]\n",
        "\n",
        "        s = 0.0\n",
        "        for k in range(na):\n",
        "            s += combined[k]\n",
        "        diff     = s / na - (combined[na:].sum()) / nb\n",
        "        abs_diff = diff if diff >= 0.0 else -diff\n",
        "\n",
        "        if abs_diff >= abs_T_obs:\n",
        "            count += 1\n",
        "\n",
        "    return T_obs, count / n_perm\n",
        "\n",
        "\n",
        "def permutation_test_centered_fast(a, b, n_perm=10_000, rng=None):\n",
        "    \"\"\"Drop-in replacement for permutation_test() â€” continuous cumrets.\"\"\"\n",
        "    seed = int(rng.integers(0, 2**31)) if rng is not None else 42\n",
        "    return _permutation_test_centered_numba(\n",
        "        a.astype(np.float64),\n",
        "        b.astype(np.float64),\n",
        "        n_perm,\n",
        "        seed\n",
        "    )\n",
        "\n",
        "@njit\n",
        "def _permutation_test_standard_numba(a, b, n_perm, seed):\n",
        "    \"\"\"Standard (non-centered) permutation test for binary win/loss data (tests p1 = p2).\"\"\"\n",
        "    na = len(a)\n",
        "    nb = len(b)\n",
        "    n  = na + nb\n",
        "\n",
        "    # no centering â€” pool raw 0/1 values\n",
        "    combined = np.empty(n)\n",
        "    for i in range(na):\n",
        "        combined[i] = a[i]\n",
        "    for i in range(nb):\n",
        "        combined[na + i] = b[i]\n",
        "\n",
        "    T_obs     = a.mean() - b.mean()\n",
        "    abs_T_obs = T_obs if T_obs >= 0.0 else -T_obs  # abs(T_obs)\n",
        "\n",
        "    rng_state = seed\n",
        "    count = 0\n",
        "\n",
        "    for _ in range(n_perm):\n",
        "        for i in range(n - 1, 0, -1):\n",
        "            rng_state = (rng_state * 6364136223846793005 + 1442695040888963407) & 0xFFFFFFFFFFFFFFFF\n",
        "            j = rng_state % (i + 1)\n",
        "            combined[i], combined[j] = combined[j], combined[i]\n",
        "\n",
        "        s = 0.0\n",
        "        for k in range(na):\n",
        "            s += combined[k]\n",
        "        diff     = s / na - (combined[na:].sum()) / nb\n",
        "        abs_diff = diff if diff >= 0.0 else -diff\n",
        "\n",
        "        if abs_diff >= abs_T_obs:\n",
        "            count += 1\n",
        "\n",
        "    return T_obs, count / n_perm\n",
        "\n",
        "def permutation_test_standard_fast(a, b, n_perm=10_000, rng=None):\n",
        "    \"\"\"Permutation test for binary win-rate comparison.\"\"\"\n",
        "    seed = int(rng.integers(0, 2**31)) if rng is not None else 42\n",
        "    return _permutation_test_standard_numba(\n",
        "        a.astype(np.float64),\n",
        "        b.astype(np.float64),\n",
        "        n_perm,\n",
        "        seed\n",
        "    )"
      ],
      "metadata": {
        "id": "hX15hc1Q4HPa"
      },
      "execution_count": null,
      "outputs": [],
      "id": "hX15hc1Q4HPa"
    },
    {
      "cell_type": "markdown",
      "source": [
        "For all combinations of patterns, holding periods, mode (bullish/bearish), and trend filter\n",
        "(no filter, uptrend, downtrend) we perform the following steps:\n",
        "* Extracting the relevant cumulative returns for the pattern and the matching random series (same trend condition)\n",
        "* Testing H0_Âµ with two independent tests: a mean-centered permutation test and a Welch t-test\n",
        "* Testing H0_wr with a standard (non-centered) permutation test"
      ],
      "metadata": {
        "id": "onZtqE_qBS_c"
      },
      "id": "onZtqE_qBS_c"
    },
    {
      "cell_type": "code",
      "source": [
        "# H0 definitions\n",
        "H0_perm    = \"The mean cumulative return after a pattern signal equals the mean cumulative return after a random entry (Âµ_pattern = Âµ_random).\"\n",
        "H0_winrate = \"The win rate of pattern signals equals the win rate of random entries (p_pattern = p_random).\"\n",
        "print(f\"H0 mean:     {H0_perm}\")\n",
        "print(f\"H0 win rate: {H0_winrate}\\n\")\n",
        "\n",
        "# parameters\n",
        "ALPHA = 0.05\n",
        "N_PERM        = 10_000\n",
        "selected_days = [5, 10, 15, 20]\n",
        "day_cols_sel  = [f'Day_{d:02d}' for d in selected_days]\n",
        "\n",
        "# Analysing all combinations in all selected days\n",
        "results_all = []\n",
        "\n",
        "for day_col, d in zip(day_cols_sel, selected_days):\n",
        "    for comb in combinations:\n",
        "        mode_value  = comb[\"mode\"]\n",
        "        trend_value = comb[\"trend\"]\n",
        "        desc        = comb[\"description\"]\n",
        "\n",
        "        # for current day select the relevant random series and get cumrets and winrate\n",
        "        cumrets_random = get_random_cumrets(analytics_rnd_all, trend_value, day_col).to_numpy()\n",
        "        wins_random    = win_condition(cumrets_random, mode_value).astype(float)\n",
        "        wr_random      = wins_random.mean()\n",
        "\n",
        "        # statistical tests\n",
        "        for pattern_name in tqdm(top_patterns_talib.index, desc=f\"Day {d} | {desc}\", unit=\"pattern\"):\n",
        "            # for current day select the relevant cumrets of patterns (by mode and trend) from top-patterns\n",
        "            cumrets_pattern = get_cumrets_for_pattern(\n",
        "                analytics, pattern_name, mode_value, trend_value, day_col\n",
        "            ).to_numpy()\n",
        "            n_pat = len(cumrets_pattern)\n",
        "            if n_pat < 10:\n",
        "                continue\n",
        "\n",
        "            # Mean permutation test\n",
        "            T_obs, p_perm = permutation_test_centered_fast(cumrets_pattern, cumrets_random, n_perm=N_PERM, rng=rng) # numba compiled version\n",
        "            # T_obs, p_perm = permutation_test_centered(cumrets_pattern, cumrets_random, n_perm=N_PERM, rng=rng)        # vectorized version\n",
        "\n",
        "            # Welch t-test\n",
        "            _, p_welch = stats.ttest_ind(cumrets_pattern, cumrets_random, equal_var=False)\n",
        "\n",
        "            # Win-rate permutation test\n",
        "            wins_pattern = win_condition(cumrets_pattern, mode_value).astype(float)\n",
        "            wr_pattern   = wins_pattern.mean()\n",
        "            wr_obs, p_winrate = permutation_test_standard_fast(wins_pattern, wins_random, n_perm=N_PERM, rng=rng)  # numba-compiled version\n",
        "            # wr_obs, p_winrate = permutation_test_standard(wins_pattern, wins_random, n_perm=N_PERM, rng=rng)         # vectorized version\n",
        "            results_all.append({\n",
        "                'day'            : d,\n",
        "                'pattern'        : pattern_name,\n",
        "                'mode'           : mode_value,\n",
        "                'trend'          : trend_value,\n",
        "                'n_pattern'      : n_pat,\n",
        "                'n_random'       : len(cumrets_random),\n",
        "                'data_quality'   : data_quality(n_pat),\n",
        "                'T_obs'          : round(T_obs,      6),\n",
        "                'p_perm'         : round(p_perm,     4),\n",
        "                'verdict_perm'   : 'Reject H0' if p_perm    < ALPHA else 'Fail to reject H0',\n",
        "                'p_welch'        : round(p_welch,    4),\n",
        "                'verdict_welch'  : 'Reject H0' if p_welch   < ALPHA else 'Fail to reject H0',\n",
        "                'wr_pattern'     : round(wr_pattern, 4),\n",
        "                'wr_random'      : round(wr_random,  4),\n",
        "                'wr_diff'        : round(wr_obs,     4),\n",
        "                'wr_direction'   : wr_direction(wr_obs),\n",
        "                'p_winrate'      : round(p_winrate,  4),\n",
        "                'verdict_winrate': 'Reject H0' if p_winrate < ALPHA else 'Fail to reject H0',\n",
        "            })\n",
        "\n",
        "# Build results DataFrame\n",
        "perm_results = (\n",
        "    pd.DataFrame(results_all)\n",
        "    .sort_values(['day', 'mode', 'trend', 'p_perm'])\n",
        "    .reset_index(drop=True)\n",
        ")"
      ],
      "metadata": {
        "id": "NfynhEiPdzPG"
      },
      "execution_count": null,
      "outputs": [],
      "id": "NfynhEiPdzPG"
    },
    {
      "cell_type": "markdown",
      "source": [
        "From the results we extract, for each holding period, all patterns where at least one test\n",
        "rejects H0_Âµ â€” i.e. the mean cumulative return after the pattern signal is statistically\n",
        "significantly different from the random benchmark.\n",
        "\n",
        "However, rejection of H0_Âµ alone does not indicate whether the deviation is favorable or\n",
        "unfavorable. A significant difference can be positive or negative. We therefore also examine\n",
        "T_obs (the observed mean difference):\n",
        "\n",
        "* **Rejection of H0_Âµ AND T_obs > 0** â†’ the pattern signal produces statistically significantly\n",
        "higher cumulative returns than random entries â€” a genuine edge\n",
        "* **Rejection of H0_Âµ AND T_obs < 0** â†’ the pattern signal produces statistically significantly\n",
        "lower cumulative returns than random entries â€” a contrarian signal\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "0AgLfO_mDCso"
      },
      "id": "0AgLfO_mDCso"
    },
    {
      "cell_type": "code",
      "source": [
        "# check for patterns where equal means hypothesis is rejected\n",
        "cols = [\n",
        "    'day', 'pattern', 'mode', 'trend', 'n_pattern', 'data_quality',\n",
        "    'T_obs', 'p_perm', 'verdict_perm', 'p_welch', 'verdict_welch']\n",
        "\n",
        "mask_quality = perm_results['data_quality'] != 'indication only'\n",
        "mask_mean    = (\n",
        "    (perm_results['verdict_perm']  == 'Reject H0') |\n",
        "    (perm_results['verdict_welch'] == 'Reject H0')\n",
        ")\n",
        "perm_rejected = (\n",
        "    perm_results[mask_quality & mask_mean]\n",
        "    .sort_values(['day', 'mode', 'trend', 'p_perm'])\n",
        "    .reset_index(drop=True)\n",
        ")\n",
        "\n",
        "# Output section\n",
        "print(f\"H0 mean: {H0_perm}\\n\")\n",
        "print(f\"Patterns where at least one test rejects H0 (excl. indication only): \"\n",
        "      f\"{len(perm_rejected)} / {len(perm_results)} tested\\n\")\n",
        "\n",
        "for d in selected_days:\n",
        "    subset = perm_rejected[perm_rejected['day'] == d].reset_index(drop=True)\n",
        "    print(f\"\\nâ”€â”€ Day {d} â”€â”€ {len(subset)} rejection(s)\")\n",
        "    display(subset[cols])"
      ],
      "metadata": {
        "id": "enSuDNaFdzSM"
      },
      "execution_count": null,
      "outputs": [],
      "id": "enSuDNaFdzSM"
    },
    {
      "cell_type": "markdown",
      "source": [
        "The results show all combinations of holding period, pattern, mode, and trend filter where\n",
        "the mean cumulative return after a pattern signal is statistically significantly higher than\n",
        "after random entries.\n",
        "\n",
        "While some patterns do show a positive effect, the magnitude of improvement is modest.\n",
        "Putting this in context of occurrence counts: the strongest observed outperformance (0.78%) belongs to CDLMARABOZU, but with only 90 occurrences over 25 years this is a weak statistical basis. The remaining outperforming patterns show improvements ranging from 0.4%\n",
        "to 0.7% â€” small effects that warrant cautious interpretation."
      ],
      "metadata": {
        "id": "oltlwJoNFv3O"
      },
      "id": "oltlwJoNFv3O"
    },
    {
      "cell_type": "code",
      "source": [
        "# compact view of rows of interest (rejection H0_Âµ and  T_obs >0)\n",
        "mask_t_obs = perm_results['T_obs'] > 0\n",
        "mask_mean_reject = (perm_results['verdict_perm'] == 'Reject H0') | (perm_results['verdict_welch'] == 'Reject H0')\n",
        "mask_data_quality = (perm_results['data_quality'] == 'good') | (perm_results['data_quality'] == 'acceptable')\n",
        "\n",
        "final_mask = mask_t_obs & mask_mean_reject & mask_data_quality\n",
        "cols_to_display = ['day', 'pattern', 'mode', 'trend', 'n_pattern', 'data_quality', 'T_obs', 'p_perm', 'verdict_perm', 'p_welch', 'verdict_welch']\n",
        "perm_results[final_mask][cols_to_display]"
      ],
      "metadata": {
        "id": "ix9-aIX0En6f"
      },
      "execution_count": null,
      "outputs": [],
      "id": "ix9-aIX0En6f"
    },
    {
      "cell_type": "code",
      "source": [
        "# check for patterns where win-rate is significantly different from win-rate of random entries\n",
        "cols = [\n",
        "    'day', 'pattern', 'mode', 'trend', 'n_pattern', 'data_quality','T_obs',\n",
        "    'wr_pattern', 'wr_random', 'wr_diff', 'wr_direction', 'verdict_winrate',\n",
        "]\n",
        "mask_quality = perm_results['data_quality'] != 'indication only'\n",
        "mask_winrate = perm_results['verdict_winrate'] == 'Reject H0'\n",
        "\n",
        "perm_rejected_wr = (\n",
        "    perm_results[mask_quality & mask_winrate]\n",
        "    .sort_values(['day', 'mode', 'trend', 'p_winrate'])\n",
        "    .reset_index(drop=True)\n",
        ")\n",
        "\n",
        "print(f\"H0 win rate: {H0_winrate}\\n\")\n",
        "print(f\"Patterns where test rejects H0 (excl. indication only): \"\n",
        "      f\"{len(perm_rejected_wr)} / {len(perm_results)} tested\\n\")\n",
        "\n",
        "for d in selected_days:\n",
        "    subset = perm_rejected_wr[perm_rejected_wr['day'] == d].reset_index(drop=True)\n",
        "    print(f\"\\nâ”€â”€ Day {d} â”€â”€ {len(subset)} rejection(s)\")\n",
        "    display(subset[cols])"
      ],
      "metadata": {
        "id": "QqEQ5QxD3phf"
      },
      "execution_count": null,
      "outputs": [],
      "id": "QqEQ5QxD3phf"
    },
    {
      "cell_type": "markdown",
      "source": [
        "We now filter for a compact overview of all combinations of holding period, pattern, mode,\n",
        "and trend filter where the win rate after a pattern signal is statistically significantly\n",
        "different from the win rate of the corresponding random series.\n",
        "\n",
        "A clear pattern emerges: in every case where H0_wr is rejected, the win rate of the pattern\n",
        "signal is **lower** than the win rate of the random benchmark â€” consistent with the contrarian\n",
        "behavior already observed in the mean cumulative return analysis."
      ],
      "metadata": {
        "id": "bM6te4rvJ55C"
      },
      "id": "bM6te4rvJ55C"
    },
    {
      "cell_type": "code",
      "source": [
        "perm_results[perm_results.verdict_winrate == 'Reject H0'][['day','pattern', 'mode', 'trend','wr_pattern', 'wr_random', 'wr_diff', 'wr_direction', 'p_winrate', 'verdict_winrate']]"
      ],
      "metadata": {
        "id": "AkJUGN63qN51"
      },
      "execution_count": null,
      "outputs": [],
      "id": "AkJUGN63qN51"
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Z7dRvMOR-cm8"
      },
      "execution_count": null,
      "outputs": [],
      "id": "Z7dRvMOR-cm8"
    },
    {
      "cell_type": "markdown",
      "source": [
        "## preparation for GitHubUpload"
      ],
      "metadata": {
        "id": "acGv8yw2-ePj"
      },
      "id": "acGv8yw2-ePj"
    }
  ]
}